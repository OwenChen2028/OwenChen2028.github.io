<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Owen Chen's Website</title>
    <style>
        body {
            font-family: Georgia, serif;
            max-width: 720px;
            margin: 60px auto;
            padding: 0 20px;
            line-height: 1.6;
            color: #222;
            background-color: #fefefe;
        }

        h1 {
            font-size: 2em;
            margin-bottom: 0.2em;
            text-align: center;
        }

        h2 {
            font-size: 1.3em;
            margin-top: 2em;
            padding-bottom: 0.2em;
            border-bottom: 1px solid #ddd;
        }

        p,
        li {
            margin-bottom: 1em;
        }

        ul {
            padding-left: 1.2em;
        }

        a {
            color: #0066cc;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .contact {
            text-align: center;
            margin: 1em 0;
            font-size: 0.95em;
            color: #555;
        }

        img {
            display: block;
            margin: 20px auto;
            border-radius: 50%;
            width: 150px;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
        }

        strong {
            color: #333;
        }

        em {
            color: #666;
            font-style: normal;
        }
    </style>
</head>

<body>
    <h1>Owen Chen</h1>
    <img src="OwenChen_PFP.jpg" alt="Owen Chen">

    <div class="contact">
        Email: <a href="mailto:owenchen2028@gmail.com">owenchen2028 [at] gmail [dot] com</a><br>
        <a href="OwenChen_CV.pdf" target="_blank">View CV</a> &middot;
        <a href="https://github.com/OwenChen2028" target="_blank">View GitHub</a>
    </div>

    <h2>About Me</h2>
    <p>
        I'm an undergraduate at Dartmouth College majoring in Computer Science.
        I have a strong interest in artificial intelligence (both applied and theoretical) and human-computer interaction (with a focus on AR/VR technologies).
        Currently, I'm actively seeking research opportunities.
    </p>

    <h2>Experience</h2>
    <ul>
        <li>
            <strong>Research Assistant</strong> – DREAM Studio, Dartmouth College<br>
            <em>September 2024 – Present</em><br>
            Contributing to <a href="https://dreamstudio.dartmouth.edu/2023/01/introducing-deep-screens/" target="_blank">Deep Screens</a>, a Mellon Foundation grant project that seeks to extract information about actor poses from movies using machine learning and utilize that information to create a 3D representation of the film viewable in a VR environment. Deep Screens studies the differences in people's perceptions of the transformed movie compared to the original, as well how acting styles have changed over time.<br>
            Contributing to <a href="https://hop.dartmouth.edu/2024-arts-integration-projects" target="_blank">44,000,000,000 Moments of Joy</a>, a Hopkins Center grant project that attempts to flip the dynamic between humans and generative AI. The project combines object detection with LLM generation to produce artistic performance instructions that serve as “prompts for humans,” which are meant to facilitate human creative expression rather than replace it, as well as encourage active, rather than passive, engagement on social media.
        </li>
        <li>
            <strong>Section Leader, COSC 31: Algorithms</strong> – Dartmouth College<br>
            <em>June 2025 – Present</em><br>
            Regularly host office hours and grade coursework.
        </li>
        <li>
            <strong>Software Engineering Intern</strong> – HelloHost<br>
            <em>February 2024 – August 2024</em><br>
            Developed backend systems and agentic workflows.
        </li>
    </ul>
	
	<h2>Projects</h2>
	<ul>
		<li>
			<strong>VivaSign</strong>  – HackDartmouth X<br>
			<em>April 26-27, 2025</em><br>
			An interactive ASL learning tool that evaluates the accuracy of users’ attempts at sign language by analyzing each keyframe of their sign according to the "5 parameters of ASL" (handshape, location, movement, palm orientation, and non-manual markers). The user is given personalized feedback through Gemini. The project won the sponsor prize, “Best Use of Gemini API.”<br>
			<a href="https://devpost.com/software/vivasign" target="_blank">View Devpost</a>
		</li>
		<li>
			<strong>The Conservation Principle</strong>  – GMTK Game Jam 2024<br>
			<em>August 16-20, 2024</em><br>
			A 2D physics-based puzzle platformer ranking in the top 7.5% of all GMTK 2024 games and in the top 200 of games in the enjoyment category out of ~7600 submissions. <br>
			<a href="https://itch.io/jam/gmtk-2024/rate/2907300" target="_blank">View Submission</a>
		</li>
	</ul>

    <h2>Coursework</h2>
    <p>
        Algorithms &middot; Programming Languages &middot; Systems Programming (Reading Course) &middot; Object-Oriented Programming &middot; Discrete Math &middot; Linear Algebra &middot; Digital Electronics
    </p>

    <h2>Certificates</h2>
    <ul>
        <li>
            <strong>Advanced Learning Algorithms</strong> – DeepLearning.AI<br>
            <a href="https://www.coursera.org/account/accomplishments/verify/U8BQSU9SSUBN" target="_blank">View Certificate</a>
        </li>
        <li>
            <strong>Supervised Machine Learning: Regression and Classification</strong> – DeepLearning.AI<br>
            <a href="https://www.coursera.org/account/accomplishments/verify/BF2DQTP6H7UH" target="_blank">View Certificate</a>
        </li>
    </ul>
</body>

</html>
